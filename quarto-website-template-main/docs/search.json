[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "In the heart failure research team, we conduct several studies related to how different surgical or medical conditions impact cardiac surgery outcomes."
  },
  {
    "objectID": "projects/index.html#research-fellowship",
    "href": "projects/index.html#research-fellowship",
    "title": "Projects",
    "section": "",
    "text": "In the heart failure research team, we conduct several studies related to how different surgical or medical conditions impact cardiac surgery outcomes."
  },
  {
    "objectID": "projects/index.html#mph-program",
    "href": "projects/index.html#mph-program",
    "title": "Projects",
    "section": "MPH program",
    "text": "MPH program\nHere I work on different projects including statistical reports on R, study designs, and patent application that help with fostering innovation."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\n\nWelcome\nMy name is Jose and I am continuing my medical career in Cleveland, Ohio. I am a researcher at the Cleveland clinic and a student at the Master of Public Health at Case Western Reserve University.My research include heart failure and public health. I am focused on large-scale assessment of cardiac surgical outcomes from a public health perspective.\nOn this website, you will find a selection of the work I am involved with. You will also find news and updates on my current projects, as well as information on upcoming events. I invite you to explore my website and learn more about my life’s work as a scientist and entrepreneur. If I am not working, I enjoy playing tennis, listening to music and spending time with friends and family.\nPlease feel free to contact me if you have any questions or would like to discuss potential projects."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Reflection on “How To Be A Modern Scientist” by Jeff Leek 3/25/2025\nThe peer review process is a critical component of scientific publishing, ensuring that research is evaluated by experts before dissemination. In How to Be a Modern Scientist, Jeff Leek provides a practical perspective on how peer review functions and the responsibilities of reviewers. One of the key takeaways from his discussion is the importance of timeliness—reviewers should either complete a review promptly or decline the request to avoid delaying the publication process. This aligns with the broader goal of improving the efficiency of scientific communication.\nAnother significant point in the book is the structure of a well-written review. Leek outlines that an effective review should contain a clear summary of the paper (motivation, methods, results) in the reviewer’s own words, a list of major issues that could affect the validity of the research, a list of minor issues that could improve clarity and presentation, and a list of typos. This structured approach helps authors address concerns effectively while maintaining the integrity of the review process.\nLeek also warns against common pitfalls in peer review, such as overly harsh criticism or sarcasm, a recommendation of whether to accept or reject the paper, unnecessary demands for additional citations, and requests for experiments/simulations that are unnecessary to justify the main points in the paper. Instead, he advocates for constructive feedback that helps improve the paper rather than simply pointing out flaws. He emphasizes that a reviewer’s role is not to rewrite the paper but to assess whether the research question, methods, and conclusions are valid and well-supported.\nAdditionally, the book discusses how scientists can get credit for their reviewing work. Platforms like Publons now allow reviewers to document their contributions. This can be particularly beneficial for early-career researchers who want to demonstrate engagement in the scientific community.\nOverall, How to Be a Modern Scientist presents peer review as a responsibility that scientists should approach with professionalism, efficiency, and fairness. Leek’s insights reinforce that while the system is not perfect, conscientious participation in peer review is essential for maintaining scientific integrity. His emphasis on transparency and efficiency in the process resonates with broader efforts to enhance scientific communication."
  },
  {
    "objectID": "blog/index.html#research-fellowship",
    "href": "blog/index.html#research-fellowship",
    "title": "Blog",
    "section": "",
    "text": "In the heart failure research team, we conduct several studies related to how different surgical or medical conditions impact cardiac surgery outcomes."
  },
  {
    "objectID": "blog/index.html#mph-program",
    "href": "blog/index.html#mph-program",
    "title": "Blog",
    "section": "MPH program",
    "text": "MPH program\nHere I work on different projects including statistical reports on R, study designs, and patent application that help with fostering innovation."
  },
  {
    "objectID": "my first analysis/index.html",
    "href": "my first analysis/index.html",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "",
    "text": "Codeknitr::opts_chunk$set(comment = NA)\nlibrary(haven)\nlibrary(janitor) \nlibrary(naniar)\nlibrary(knitr)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(car)\nlibrary(caret)\nlibrary(GGally)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(mice)\nlibrary(mosaic)\nlibrary(patchwork)\nlibrary(ROCR)\nlibrary(rsample)\nlibrary(rms)\nlibrary(cutpointr) \nlibrary(glue)\n\nlibrary(easystats)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "my first analysis/index.html#r-packages-and-setup",
    "href": "my first analysis/index.html#r-packages-and-setup",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "",
    "text": "Codeknitr::opts_chunk$set(comment = NA)\nlibrary(haven)\nlibrary(janitor) \nlibrary(naniar)\nlibrary(knitr)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(car)\nlibrary(caret)\nlibrary(GGally)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(mice)\nlibrary(mosaic)\nlibrary(patchwork)\nlibrary(ROCR)\nlibrary(rsample)\nlibrary(rms)\nlibrary(cutpointr) \nlibrary(glue)\n\nlibrary(easystats)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "my first analysis/index.html#loading-the-raw-data",
    "href": "my first analysis/index.html#loading-the-raw-data",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n3.1 Loading the Raw Data",
    "text": "3.1 Loading the Raw Data\n\nCode# &lt;https://hrsdata.isr.umich.edu/data-products/2022-hrs-core&gt;\n\nPhysMeasure_raw &lt;- read_sas(\"C:/Users/dizferj/OneDrive - Cleveland Clinic/Documents/MPH/432/Data/h22core/h22sas/h22i_r.sas7bdat\") |&gt;\n  zap_label() |&gt;\n  distinct(HHID, .keep_all = TRUE)\n\nPhysHealth_raw &lt;- read_sas(\"C:/Users/dizferj/OneDrive - Cleveland Clinic/Documents/MPH/432/Data/h22core/h22sas/h22c_r.sas7bdat\") |&gt;\n  zap_label() |&gt;\n  distinct(HHID, .keep_all = TRUE)\n\nFamStructH_raw &lt;- read_sas(\"C:/Users/dizferj/OneDrive - Cleveland Clinic/Documents/MPH/432/Data/h22core/h22sas/h22e_h.sas7bdat\") |&gt;\n  zap_label() |&gt;\n  distinct(HHID, .keep_all = TRUE)\n\nFamStructMC_raw &lt;- read_sas(\"C:/Users/dizferj/OneDrive - Cleveland Clinic/Documents/MPH/432/Data/h22core/h22sas/h22e_mc.sas7bdat\") |&gt;\n  zap_label() |&gt;\n  distinct(HHID, .keep_all = TRUE)\n\nDemographic_raw &lt;- read_sas(\"C:/Users/dizferj/OneDrive - Cleveland Clinic/Documents/MPH/432/Data/h22core/h22sas/h22b_r.sas7bdat\") |&gt;\n  zap_label() |&gt;\n  distinct(HHID, .keep_all = TRUE)\n\n\nOriginal data sets had some duplicate rows, which we removed above."
  },
  {
    "objectID": "my first analysis/index.html#cleaning-the-data",
    "href": "my first analysis/index.html#cleaning-the-data",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n3.2 Cleaning the Data",
    "text": "3.2 Cleaning the Data\n\n3.2.1 Selecting Variables\n\nCodePhysMeasure &lt;- PhysMeasure_raw |&gt;\n  select(HHID, SI859) |&gt;\n  rename(sbp = SI859) |&gt;\n  janitor::clean_names()\n\nPhysHealth &lt;- PhysHealth_raw |&gt;\n  select(HHID, SC010) |&gt;\n  rename(diabetes  = SC010) |&gt;\n  janitor::clean_names()\n\nFamStructH &lt;- FamStructH_raw |&gt;\n  select(HHID, SE022, SE046) |&gt;\n  rename(new_gchild = SE022, num_gchild = SE046) |&gt;\n  janitor::clean_names()\n\nFamStructMC &lt;- FamStructMC_raw |&gt;\n  select(HHID, SE044) |&gt;\n  rename(num_child = SE044) |&gt;\n  janitor::clean_names()\n\nDemographic &lt;- Demographic_raw |&gt;\n  select(HHID, SB063) |&gt;\n  rename(marital = SB063) |&gt;\n  janitor::clean_names()\n\n\n\n3.2.2 Cleaning Outcome Data\n\nCodePhysMeasure &lt;- PhysMeasure |&gt;\n  mutate(sbp = ifelse(sbp &gt; 1 & sbp &lt; 993, sbp, NA))\n\nPhysHealth &lt;- PhysHealth |&gt;\n  mutate(diabetes = ifelse(diabetes &gt;= 1 & diabetes &lt;= 6, diabetes, NA)) |&gt;\n  mutate(diabetes = as.factor(diabetes)) |&gt;\n  mutate(diabetes = diabetes |&gt; \n           fct_recode(\"Yes\" = \"1\", \"Yes\" = \"6\", \"No\" = \"4\", \"No\" = \"5\"))\n\n\n\n3.2.3 Cleaning Predictor Data\n\nCodeFamStructH &lt;- FamStructH |&gt;\n  mutate(new_gchild = ifelse(new_gchild &gt;= 1 & new_gchild &lt;= 8, new_gchild, NA), \n         num_gchild = ifelse(num_gchild &gt;= 0 & num_gchild &lt;= 80, num_gchild, NA)) |&gt;\n  mutate(new_gchild = as.factor(new_gchild)) |&gt;\n  mutate(new_gchild = new_gchild |&gt; \n           fct_recode(\"Yes\" = \"1\", \"No\" = \"5\", \"No\" = \"8\"))\n\nFamStructMC &lt;- FamStructMC |&gt;\n  mutate(num_child = ifelse(num_child &gt;= 0 & num_child &lt;= 20, num_child, NA))\n\nDemographic &lt;- Demographic |&gt;\n  mutate(marital = ifelse(marital &gt;= 1 & marital &lt;= 6, marital, NA)) |&gt;\n  mutate(marital = as.factor(marital)) |&gt;\n  mutate(marital = marital |&gt; \n           fct_recode(\"Married\" = \"1\", \"Seperated\" = \"2\", \"Seperated\" = \"3\",\n                      \"Seperated\" = \"4\", \"Widowed\" = \"5\", \"Never Married\" = \"6\"))\n\n\n\n3.2.4 Combined Tibble\n\nCodetemp1 &lt;- full_join(Demographic, FamStructH, by = \"hhid\")\ntemp2 &lt;- full_join(temp1, FamStructMC, by = \"hhid\")\ntemp3 &lt;- full_join(temp2, PhysMeasure, by = \"hhid\")\nPhysTibble &lt;- full_join(temp3, PhysHealth, by = \"hhid\")\n\ndim(PhysTibble)\n\n[1] 11542     7\n\nCodePhysTibble &lt;- PhysTibble |&gt; \n  drop_na()\n\ndim(PhysTibble)\n\n[1] 2231    7\n\nCodeset.seed(432)\nPhysTibble &lt;- PhysTibble |&gt;\n  slice_sample(n=2000)\n\ndim(PhysTibble)\n\n[1] 2000    7\n\n\nAs it can be seen above, we were able to filter for complete cases across all variables with more than enough samples left over. We then sampled the complete cases tibble for 2000 cases to meet the specifications. Originally we made two separate tibbles for our Linear and Logistic Regression. Although this would work, we have more than enough complete cases to study the the same sample of people for both models. This might be interesting considering both models use the same inputs to predict a physical health outcome, so we decided on a combined tibble containing both outcomes."
  },
  {
    "objectID": "my first analysis/index.html#listing-the-tibble",
    "href": "my first analysis/index.html#listing-the-tibble",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n4.1 Listing the Tibble",
    "text": "4.1 Listing the Tibble\n\nCodePhysTibble\n\n# A tibble: 2,000 × 7\n   hhid   marital   new_gchild num_gchild num_child   sbp diabetes\n   &lt;chr&gt;  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 050380 Widowed   No                  5         2   153 Yes     \n 2 501961 Married   No                 16         0   107 Yes     \n 3 543841 Seperated Yes                21         2   164 No      \n 4 014092 Widowed   No                  4         3   154 No      \n 5 022335 Widowed   No                  9         3   137 No      \n 6 537264 Married   Yes                 6         1   142 No      \n 7 903224 Seperated No                  9         1   136 Yes     \n 8 500094 Married   No                  4         2   153 No      \n 9 062157 Widowed   No                 10         1   133 No      \n10 077723 Widowed   No                  5         3   142 No      \n# ℹ 1,990 more rows"
  },
  {
    "objectID": "my first analysis/index.html#size-and-identifiers",
    "href": "my first analysis/index.html#size-and-identifiers",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n4.2 Size and Identifiers",
    "text": "4.2 Size and Identifiers\n\nCodedim(PhysTibble)\n\n[1] 2000    7\n\nCoden_distinct(PhysTibble$hhid)\n\n[1] 2000\n\nCodeclass(PhysTibble$hhid)\n\n[1] \"character\"\n\n\nOur final tibble has 2000 samples with 7 variables: an identifier, four predictors, and two outcomes. The identifier is defined as a character in R and is named hhid. It has 2000 distinct values which matches the sample size of our final tibble."
  },
  {
    "objectID": "my first analysis/index.html#save-the-tibble",
    "href": "my first analysis/index.html#save-the-tibble",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n4.3 Save The Tibble",
    "text": "4.3 Save The Tibble\n\nCodewrite_rds(PhysTibble, file = \"PhysTibble_reillyJose.Rds\")"
  },
  {
    "objectID": "my first analysis/index.html#defining-the-variables",
    "href": "my first analysis/index.html#defining-the-variables",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n5.1 Defining the Variables",
    "text": "5.1 Defining the Variables\n\n\n\n\n\n\n\n\nVariable\nRole\nType\nDescription\n\n\n\nhhid\nIdentifier\n-\ncharacter code for respondents\n\n\nmarital\nInput\n4-cat\nMarital Status (Married, Separated, Widowed, Never Married)\n\n\nnew_gchild\nInput\n2-cat\nAny new grandchildren in past 2 years (Yes/No)\n\n\nnum_gchild\nInput\nquant\nNumber of grandchildren of respondent\n\n\nnum_child\nInput\nquant\nNumber of children of respondent\n\n\nsbp\nOutcome (linear)\nquant\nMeasured Systolic Blood Pressure (mmHg)\n\n\ndiabetes\nOutcome (logistic)\n2-cat\nHave or had Diabetes or High Blood Pressure (Yes/No)"
  },
  {
    "objectID": "my first analysis/index.html#numerical-description",
    "href": "my first analysis/index.html#numerical-description",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n5.2 Numerical Description",
    "text": "5.2 Numerical Description\n\nCodedata_codebook(PhysTibble, max_values = 3)\n\nPhysTibble (2000 rows and 7 variables, 7 shown)\n\nID | Name       | Type        | Missings |        Values |            N\n---+------------+-------------+----------+---------------+-------------\n1  | hhid       | character   | 0 (0.0%) |        010038 |    1 ( 0.0%)\n   |            |             |          |        010059 |    1 ( 0.0%)\n   |            |             |          |        010451 |    1 ( 0.0%)\n   |            |             |          |         (...) |             \n---+------------+-------------+----------+---------------+-------------\n2  | marital    | categorical | 0 (0.0%) |       Married |  837 (41.9%)\n   |            |             |          |     Seperated |  466 (23.3%)\n   |            |             |          |       Widowed |  570 (28.5%)\n   |            |             |          | Never Married |  127 ( 6.3%)\n---+------------+-------------+----------+---------------+-------------\n3  | new_gchild | categorical | 0 (0.0%) |           Yes |  402 (20.1%)\n   |            |             |          |            No | 1598 (79.9%)\n---+------------+-------------+----------+---------------+-------------\n4  | num_gchild | numeric     | 0 (0.0%) |       [0, 45] |         2000\n---+------------+-------------+----------+---------------+-------------\n5  | num_child  | numeric     | 0 (0.0%) |       [0, 20] |         2000\n---+------------+-------------+----------+---------------+-------------\n6  | sbp        | numeric     | 0 (0.0%) |     [74, 232] |         2000\n---+------------+-------------+----------+---------------+-------------\n7  | diabetes   | categorical | 0 (0.0%) |           Yes |  661 (33.1%)\n   |            |             |          |            No | 1339 (67.0%)\n-----------------------------------------------------------------------"
  },
  {
    "objectID": "my first analysis/index.html#my-first-research-question",
    "href": "my first analysis/index.html#my-first-research-question",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n6.1 My First Research Question",
    "text": "6.1 My First Research Question\nDoes an increased presence of children and grandchildren directly affect a person’s Systolic Blood Pressure while accounting for marital status?"
  },
  {
    "objectID": "my first analysis/index.html#my-quantitative-outcome",
    "href": "my first analysis/index.html#my-quantitative-outcome",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n6.2 My Quantitative Outcome",
    "text": "6.2 My Quantitative Outcome\nOur quantitative outcome is named sbp in our final tibble. We are interested in this variable since adults over 50 have an increased risk of high Systolic Blood Pressure (SBP). We were interested in seeing if family structure data, such as marital status and the number direct descendants of a person, result in lower SBP measurements for survey respondents. All rows of our final tibble contain complete data on sbp.\n\nCoden_distinct(PhysTibble$sbp)\n\n[1] 120\n\nCodebw = 2 \nplot1 &lt;- ggplot(PhysTibble, aes(x = sbp)) +                     \n  geom_histogram(binwidth = bw, fill = \"black\", col = \"white\") + \n  stat_function(fun = function(x)                               \n    dnorm(x, mean = mean(PhysTibble$sbp, na.rm = TRUE),         \n          sd = sd(PhysTibble$sbp, na.rm = TRUE)) * \n          length(PhysTibble$sbp) * bw,\n    geom = \"area\", alpha = 0.5, \n    fill = \"lightblue\", col = \"blue\") +                         \n  labs(x = \"Systolic Blood Pressure (SBP)\", y = \"Count\")\n\nplot2 &lt;- ggplot(PhysTibble, aes(sample = sbp)) +\n  geom_qq(color = \"black\") +\n  geom_qq_line(color = \"blue\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  labs(x = \"Theoretical Quantiles\", y = \"Sample Quantiles\")\n\nplot1 + plot2 +\n  plot_annotation(title = \"Simple Visualizations of Distribution of Systolic Blood Presure\",\n                  subtitle = \"Histogram and Q-Q Plot\")\n\n\n\n\n\n\n\nThe plot above shows the distribution of sbp measurements in this sample is relatively symmetric and therefore almost continuous. The fitted normal curve models the distribution well suggesting a transform is most likely not necessary. Using the “n_distinct” function it can be seen that sbp has 120 different values which meets the specifications.\n\nCodedescribe(PhysTibble$sbp)\n\nPhysTibble$sbp \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n    2000        0      120        1    130.7      130    22.87      101 \n     .10      .25      .50      .75      .90      .95 \n     106      117      129      143      158      167 \n\nlowest :  74  79  81  83  84, highest: 206 207 213 223 232\n\nCodefavstats(PhysTibble$sbp) |&gt; gt()\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n74\n117\n129\n143\n232\n130.7185\n20.60861\n2000\n0\n\n\n\n\nCodePhysTibble |&gt;\n  tabyl(sbp) |&gt;\n  adorn_pct_formatting() |&gt; \n  arrange(desc(n)) |&gt;\n  head(5) |&gt;\n  gt()\n\n\n\n\n\nsbp\nn\npercent\n\n\n\n130\n57\n2.9%\n\n\n136\n50\n2.5%\n\n\n126\n48\n2.4%\n\n\n127\n47\n2.4%\n\n\n119\n45\n2.2%\n\n\n\n\n\n\nThe tables above show that sbp has a mean of around 130 mmHg, which is close to the highest percentage instance of sbp. This value makes up 2.9% of the outcome data for these 2000 samples which is below the 10% threshold."
  },
  {
    "objectID": "my first analysis/index.html#my-planned-predictors-linear-model",
    "href": "my first analysis/index.html#my-planned-predictors-linear-model",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n6.3 My Planned Predictors (Linear Model)",
    "text": "6.3 My Planned Predictors (Linear Model)\n\nCoden_distinct(PhysTibble$num_gchild)\n\n[1] 34\n\nCoden_distinct(PhysTibble$num_child)\n\n[1] 13\n\nCodesummary_table &lt;- PhysTibble |&gt; \n  select(new_gchild, marital) |&gt; \n  map_df(~ tabyl(.) |&gt; adorn_totals(\"row\"), .id = \"Variable\") \n\nsummary_table |&gt; \n  gt() |&gt; \n  tab_header(title = \"Summary of Categorical Variables\") |&gt; \n  fmt_number(columns = c(\"percent\"), decimals = 3) |&gt; \n  cols_label(n = \"Count\", percent = \"Percent\")\n\n\n\n\n\n\nSummary of Categorical Variables\n\n\nVariable\n.\nCount\nPercent\n\n\n\n\nnew_gchild\nYes\n402\n0.201\n\n\nnew_gchild\nNo\n1598\n0.799\n\n\nnew_gchild\nTotal\n2000\n1.000\n\n\nmarital\nMarried\n837\n0.418\n\n\nmarital\nSeperated\n466\n0.233\n\n\nmarital\nWidowed\n570\n0.285\n\n\nmarital\nNever Married\n127\n0.064\n\n\nmarital\nTotal\n2000\n1.000\n\n\n\n\n\n\nWe have four predictors total as shown in the codebook above. The two that are quantitative are num_gchild and num_child with 34 and 13 distinct values respectively. Our categorical variables are new_gchild, which is binary, and marital which is four levels. The table above shows greater than 30 observations for each variable. Our outcome sbp has no missing data and therefore the candidate predictors must not be any bigger than 4 + (2000-100)/100 = 23. With only four predictors, we are well below this threshold.\n\n6.3.1 Model Expectations\nWe expect to see lower Systolic Blood Pressure (SBP) Levels in those with more grandchildren and those who recently had grand children. For direct children we expect to see a similar outcome as to grandchildren, but possibly not as strong. We also believe there might be an association between SBP and the marital status of a person, while expecting a difference between those who are married and those who are separated.\n\n6.3.2 Missingness Summary\n\nCodemiss_var_summary(PhysTibble)\n\n# A tibble: 7 × 3\n  variable   n_miss pct_miss\n  &lt;chr&gt;       &lt;int&gt;    &lt;num&gt;\n1 hhid            0        0\n2 marital         0        0\n3 new_gchild      0        0\n4 num_gchild      0        0\n5 num_child       0        0\n6 sbp             0        0\n7 diabetes        0        0\n\nCodemiss_case_table(PhysTibble)\n\n# A tibble: 1 × 3\n  n_miss_in_case n_cases pct_cases\n           &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;\n1              0    2000       100\n\n\nFrom the summaries above it can be seen that we have absolutely no missing data. This is explained above in the data cleaning section."
  },
  {
    "objectID": "my first analysis/index.html#my-second-research-question",
    "href": "my first analysis/index.html#my-second-research-question",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n7.1 My Second Research Question",
    "text": "7.1 My Second Research Question\nIs there an increased risk for diabetes in those with either less children or grandchildren and how does this change when factoring in a person’s current marital status?"
  },
  {
    "objectID": "my first analysis/index.html#my-binary-outcome",
    "href": "my first analysis/index.html#my-binary-outcome",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n7.2 My Binary Outcome",
    "text": "7.2 My Binary Outcome\n\nCodePhysTibble |&gt; tabyl(diabetes)\n\n diabetes    n percent\n      Yes  661  0.3305\n       No 1339  0.6695\n\n\nOur binary outcome is called diabetes which is a yes or no answer to whether the person aged over 50 has had diabetes in their life. The table above shows that almost one third of the sample has answered yes."
  },
  {
    "objectID": "my first analysis/index.html#my-planned-predictors-logistic-model",
    "href": "my first analysis/index.html#my-planned-predictors-logistic-model",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n7.3 My Planned Predictors (Logistic Model)",
    "text": "7.3 My Planned Predictors (Logistic Model)\nThe predictors here are the same as listed in the linear model section: num_gchild, num_child, new_gchild, and marital. N2 would be 661 since the “Yes” group is smaller. This means our model can have no more than 4+(661-100)/100 = 9 predictors, which is well beyond the planned count of four predictors for this analysis.\n\n7.3.1 Model Expectations\nWe expect to see a positive association between those with more grandchildren and children and not having diabetes. We do not have a good guess for the association between marital status and diabetes, but we do assume that the results might align with our Linear Model’s association with Systolic Blood Pressure. Children and grandchildren can take time out of a person’s life, so it will be interesting to see if that time spent is positively associated with better health.\n\n7.3.2 Anticipated Direction of Effects\nI expect higher blood pressure risk to be associated with widowed status, with no new grand children, a lower number of children or grand children.\n\n7.3.3 Missingness Summary\n\nCodemiss_var_summary(PhysTibble)\n\n# A tibble: 7 × 3\n  variable   n_miss pct_miss\n  &lt;chr&gt;       &lt;int&gt;    &lt;num&gt;\n1 hhid            0        0\n2 marital         0        0\n3 new_gchild      0        0\n4 num_gchild      0        0\n5 num_child       0        0\n6 sbp             0        0\n7 diabetes        0        0\n\nCodemiss_case_table(PhysTibble)\n\n# A tibble: 1 × 3\n  n_miss_in_case n_cases pct_cases\n           &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;\n1              0    2000       100\n\n\nFrom the summaries above it can be seen that we have absolutely no missing data. This is explained above in the data cleaning section where we chose working with complete cases.\n\nWe have complete data for all of the logistic model predictors in 2000 (100%) of the 2000 rows in my data.\nWe are missing XX values (XX%) for (insert variable name here) (optional with imputation)"
  },
  {
    "objectID": "my first analysis/index.html#missingness",
    "href": "my first analysis/index.html#missingness",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n8.1 Missingness",
    "text": "8.1 Missingness\nWe will work with complete cases since missing data was dropped in the data cleaning section.\n\nCodemiss_var_summary(PhysTibble)\n\n# A tibble: 7 × 3\n  variable   n_miss pct_miss\n  &lt;chr&gt;       &lt;int&gt;    &lt;num&gt;\n1 hhid            0        0\n2 marital         0        0\n3 new_gchild      0        0\n4 num_gchild      0        0\n5 num_child       0        0\n6 sbp             0        0\n7 diabetes        0        0\n\n\n\n8.1.1 Single Imputation Approach (optional)\nPhysTibble_i &lt;- mice(PhysTibble, m = 1, seed = 432432, print = FALSE) |&gt; complete() |&gt; tibble()\nn_miss(PhysTibble_i)"
  },
  {
    "objectID": "my first analysis/index.html#outcome-transformation",
    "href": "my first analysis/index.html#outcome-transformation",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n8.2 Outcome Transformation",
    "text": "8.2 Outcome Transformation\n\nCodemod_temp &lt;- lm(sbp ~ num_gchild + new_gchild + marital + num_child, data = PhysTibble)\n\nboxCox(mod_temp)\n\n\n\n\n\n\nCodePhysTibble &lt;- PhysTibble |&gt;\n  mutate(logsbp = log(sbp))\n\np1 &lt;- ggplot(PhysTibble, aes(sample = logsbp)) +\n  geom_qq(col = \"navy\") + geom_qq_line(col = \"red\") + \n  labs(title = \"Normal Q-Q plot of log(sbp)\", x = \"\",\n       y = \"Log of SBP Cholesterol Level (mmHg)\")\n\np2 &lt;- ggplot(PhysTibble, aes(x = logsbp)) +\n  geom_histogram(bins = 20, col = \"white\", fill = \"navy\") +\n  labs(title = \"Histogram of log(sbp)\", x = \"Log of SBP Cholesterol Level (mmHg)\")\n\np1 + p2"
  },
  {
    "objectID": "my first analysis/index.html#scatterplot-matrix-and-collinearity",
    "href": "my first analysis/index.html#scatterplot-matrix-and-collinearity",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n8.3 Scatterplot Matrix and Collinearity",
    "text": "8.3 Scatterplot Matrix and Collinearity\n\nCodeggpairs(PhysTibble, columns = c(\"num_gchild\", \"new_gchild\", \"marital\", \n                               \"num_child\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nCodemod_A &lt;- lm(sbp ~ num_gchild + new_gchild + marital + num_child, data = PhysTibble)\n\ncar::vif(mod_A)\n\n               GVIF Df GVIF^(1/(2*Df))\nnum_gchild 1.174037  1        1.083530\nnew_gchild 1.026832  1        1.013327\nmarital    1.041998  3        1.006880\nnum_child  1.177227  1        1.085001"
  },
  {
    "objectID": "my first analysis/index.html#model-a",
    "href": "my first analysis/index.html#model-a",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n8.4 Model A",
    "text": "8.4 Model A\n\n8.4.1 Fitting Model A\n\nCodemod_A &lt;- lm(logsbp ~ num_gchild + new_gchild + marital + num_child, data = PhysTibble)\n\ndd &lt;- datadist(PhysTibble)\noptions(datadist = \"dd\")\n\nmod_A_ols &lt;- ols(logsbp ~ num_gchild + new_gchild + marital + num_child,\n                 data = PhysTibble, x = TRUE, y = TRUE)\n\n\n\n8.4.2 Tidied Coefficient Estimates (Model A)\n\nCodemodel_parameters(mod_A, ci = 0.90) |&gt; print_md(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n90% CI\nt(1993)\np\n\n\n\n(Intercept)\n4.827\n0.010\n(4.810, 4.843)\n481.141\n&lt; .001\n\n\nnum gchild\n7.522e-04\n7.662e-04\n(-5.086e-04, 0.002)\n0.982\n0.326\n\n\nnew gchild (No)\n0.023\n0.009\n(0.009, 0.038)\n2.672\n0.008\n\n\nmarital (Seperated)\n-0.002\n0.009\n(-0.017, 0.013)\n-0.210\n0.834\n\n\nmarital (Widowed)\n0.012\n0.009\n(-0.002, 0.026)\n1.359\n0.174\n\n\nmarital (Never Married)\n0.006\n0.015\n(-0.018, 0.031)\n0.415\n0.678\n\n\nnum child\n0.004\n0.002\n(-8.335e-05, 0.007)\n1.608\n0.108\n\n\n\n\n\n\n8.4.3 Summarizing Fit (Model A)\n\nCodeplot(summary(mod_A_ols, conf.int = 0.90))\n\n\n\n\n\n\nCodesummary(mod_A_ols, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\nnum_gchild\n3\n8\n5\n0.004\n0.004\n-0.003\n0.010\n1\n\n\nnum_child\n1\n3\n2\n0.007\n0.004\n0.000\n0.015\n1\n\n\nnew_gchild - Yes:No\n2\n1\nNA\n-0.023\n0.009\n-0.038\n-0.009\n1\n\n\nmarital - Seperated:Married\n1\n2\nNA\n-0.002\n0.009\n-0.017\n0.013\n1\n\n\nmarital - Widowed:Married\n1\n3\nNA\n0.012\n0.009\n-0.002\n0.026\n1\n\n\nmarital - Never Married:Married\n1\n4\nNA\n0.006\n0.015\n-0.018\n0.031\n1\n\n\n\n\nCodemodel_performance(mod_A) |&gt; print_md(digits = 3)\n\n\nIndices of model performance\n\nAIC\nAICc\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n-1762.165\n-1762.093\n-1717.358\n0.009\n0.006\n0.155\n0.155\n\n\n\nCodeglance(mod_A) |&gt;\n  select(r2 = r.squared, adjr2 = adj.r.squared, sigma, \n         AIC, BIC, nobs, df, df.residual) |&gt;\n  kable(digits = c(3, 3, 2, 1, 1, 0, 0, 0))\n\n\n\nr2\nadjr2\nsigma\nAIC\nBIC\nnobs\ndf\ndf.residual\n\n\n0.009\n0.006\n0.16\n-1762.2\n-1717.4\n2000\n6\n1993\n\n\n\n\n\n8.4.4 Regression Diagnostics (Model A)\nFor the most part, these plots look very reasonable. I see no clear problems with the assumptions of linearity, normality or constant variance evident in any of these results.\nThe main issue is the posterior predictive check, where our predictions are missing on the left part of the center of the distribution, with more predicted values of log(HDL) in the 112.5 to 125 range than we see in the original data.\n\nCodecheck_model(mod_A, detrend = FALSE)"
  },
  {
    "objectID": "my first analysis/index.html#non-linearity",
    "href": "my first analysis/index.html#non-linearity",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n8.5 Non-Linearity",
    "text": "8.5 Non-Linearity\nHere’s the relevant Spearman plot, as a place to look for sensible places to consider a non-linear term or terms.\n\nCodeplot(spearman2(logsbp ~ num_gchild + new_gchild + marital + num_child, data = PhysTibble))\n\n\n\n\n\n\n\nOur Spearman plot first suggests the use of a non-linear term in new_gchild, so we’ll add a restricted cubic spline in new_gchild using 5 knots, which should add 3 degrees of freedom to our initial model.\nNext, the num_gchild variable also seems to be a good choice, so we’ll add an interaction between num_gchild and the main effect of new_gchild, which will add one more degree of freedom to our model A."
  },
  {
    "objectID": "my first analysis/index.html#model-b",
    "href": "my first analysis/index.html#model-b",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n8.6 Model B",
    "text": "8.6 Model B\nOur Model B will add two non-linear terms, summing up to 4 additional degrees of freedom, to our model A.\n\n8.6.1 Fitting Model B\nWe’ll also fit model B with the ols() function from the rms package.\n\nCodemod_B &lt;- lm(logsbp ~ rcs(num_gchild, 5) + new_gchild + num_gchild %ia% new_gchild + marital, data = PhysTibble)\n\ndd &lt;- datadist(PhysTibble)\noptions(datadist = \"dd\")\n\nmod_B_ols &lt;- ols(logsbp ~ rcs(num_gchild, 5) + new_gchild + num_gchild %ia% new_gchild + marital,\n            data = PhysTibble, x = TRUE, y = TRUE)\n\n\n\n8.6.2 Tidied Coefficient Estimates (Model B)\n\nCodetidy(mod_B, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, \n         low90 = conf.low, high90 = conf.high, \n         p = p.value) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n(Intercept)\n4.826\n0.022\n4.789\n4.863\n0.000\n\n\nrcs(num_gchild, 5)num_gchild\n0.003\n0.010\n-0.013\n0.019\n0.748\n\n\nrcs(num_gchild, 5)num_gchild’\n-0.014\n0.158\n-0.275\n0.247\n0.930\n\n\nrcs(num_gchild, 5)num_gchild’’\n0.044\n0.399\n-0.613\n0.701\n0.912\n\n\nrcs(num_gchild, 5)num_gchild’’’\n-0.055\n0.311\n-0.567\n0.456\n0.859\n\n\nnew_gchildNo\n0.016\n0.014\n-0.007\n0.039\n0.252\n\n\nnum_gchild %ia% new_gchild\n0.001\n0.002\n-0.002\n0.004\n0.474\n\n\nmaritalSeperated\n0.000\n0.009\n-0.015\n0.014\n0.965\n\n\nmaritalWidowed\n0.011\n0.009\n-0.003\n0.025\n0.189\n\n\nmaritalNever Married\n0.008\n0.015\n-0.016\n0.033\n0.573\n\n\n\n\n\n\n8.6.3 Effects Plot for Model B\nHere, we use the mod_B_ols model to look at the effects using its plot and associated table, which may be especially helpful when we include non-linear terms.\n\nCodeplot(summary(mod_B_ols, conf.int = 0.90))\n\n\n\n\n\n\nCodesummary(mod_B_ols, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\nnum_gchild\n3\n8\n5\n0.018\n0.010\n0.002\n0.035\n1\n\n\nnew_gchild - Yes:No\n2\n1\nNA\n-0.022\n0.009\n-0.037\n-0.007\n1\n\n\nmarital - Seperated:Married\n1\n2\nNA\n0.000\n0.009\n-0.015\n0.014\n1\n\n\nmarital - Widowed:Married\n1\n3\nNA\n0.011\n0.009\n-0.003\n0.025\n1\n\n\nmarital - Never Married:Married\n1\n4\nNA\n0.008\n0.015\n-0.016\n0.033\n1\n\n\n\n\n\n\n8.6.4 Summarizing Fit (Model B)\n\nCodeglance(mod_B) |&gt;\n  select(r2 = r.squared, adjr2 = adj.r.squared, sigma, \n         AIC, BIC, nobs, df, df.residual) |&gt;\n  kable(digits = c(3, 3, 2, 1, 1, 0, 0, 0))\n\n\n\nr2\nadjr2\nsigma\nAIC\nBIC\nnobs\ndf\ndf.residual\n\n\n0.01\n0.005\n0.16\n-1758.6\n-1697\n2000\n9\n1990\n\n\n\n\n\n8.6.5 Regression Diagnostics (Model B)\nThese residual plots look reasonable. I see no clear problems with the assumptions of normality or constant variance evident in these results. Slight issues with linearity and heteroskedasticity are present on the left parts of the graphs. The posterior predictive check seems simital to Model A. The collinearity we’ve introduced here is due to the interaction terms, so that’s not a concern for us.\n\nCodecheck_model(mod_B, detrend = FALSE)"
  },
  {
    "objectID": "my first analysis/index.html#validating-models-a-and-b",
    "href": "my first analysis/index.html#validating-models-a-and-b",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n8.7 Validating Models A and B",
    "text": "8.7 Validating Models A and B\n\nCodeset.seed(4321); (valA &lt;- validate(mod_A_ols))\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.0085   0.0125 0.0048   0.0077          0.0008 40\nMSE           0.0241   0.0241 0.0242  -0.0001          0.0241 40\ng             0.0160   0.0190 0.0140   0.0050          0.0110 40\nIntercept     0.0000   0.0000 1.1130  -1.1130          1.1130 40\nSlope         1.0000   1.0000 0.7710   0.2290          0.7710 40\n\nCodeset.seed(4322); (valB &lt;- validate(mod_B_ols))\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.0097   0.0143 0.0030   0.0113         -0.0016 40\nMSE           0.0240   0.0242 0.0242   0.0000          0.0240 40\ng             0.0174   0.0201 0.0129   0.0073          0.0102 40\nIntercept     0.0000   0.0000 1.6476  -1.6476          1.6476 40\nSlope         1.0000   1.0000 0.6611   0.3389          0.6611 40\n\n\n\n8.7.1 Validated \\(R^2\\) statistics, and MSE as well as IC statistics\nC = 0.5 + Dxy/2\n\n\nModel\nValidated \\(R^2\\)\n\nvalidated MSR\nAIC\nBIC\ndf\n\n\n\nA\n0.0008\n0.0241\n-1762.2\n-1717.4\n6\n\n\nB\n-0.0016\n0.024\n-1758.6\n-1697\n9"
  },
  {
    "objectID": "my first analysis/index.html#final-linear-regression-model",
    "href": "my first analysis/index.html#final-linear-regression-model",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n8.8 Final Linear Regression Model",
    "text": "8.8 Final Linear Regression Model\nWe’ll choose Model A here.\nWe see worse numbers for Model B in terms of validated and validated MSR. Model A matches the assumptions of linear regression better. Given the negative R2 value of model b, we will definitly pick model a.\n\n8.8.1 Winning Model’s Parameter Estimates\n\nCodemod_A_ols\n\nLinear Regression Model\n\nols(formula = logsbp ~ num_gchild + new_gchild + marital + num_child, \n    data = PhysTibble, x = TRUE, y = TRUE)\n\n                Model Likelihood    Discrimination    \n                      Ratio Test           Indexes    \nObs    2000    LR chi2     17.15    R2       0.009    \nsigma0.1554    d.f.            6    R2 adj   0.006    \nd.f.   1993    Pr(&gt; chi2) 0.0087    g        0.016    \n\nResiduals\n\n       Min         1Q     Median         3Q        Max \n-0.5755729 -0.0998415 -0.0008637  0.1035361  0.6078322 \n\n                      Coef    S.E.   t      Pr(&gt;|t|)\nIntercept              4.8268 0.0100 481.14 &lt;0.0001 \nnum_gchild             0.0008 0.0008   0.98 0.3263  \nnew_gchild=No          0.0235 0.0088   2.67 0.0076  \nmarital=Seperated     -0.0019 0.0090  -0.21 0.8340  \nmarital=Widowed        0.0116 0.0085   1.36 0.1743  \nmarital=Never Married  0.0062 0.0149   0.41 0.6782  \nnum_child              0.0036 0.0022   1.61 0.1079  \n\n\n\n8.8.2 Effects Plot for Winning Model\n\nCodeplot(summary(mod_A_ols, conf.int = 0.90))\n\n\n\n\n\n\n\n\n8.8.3 Numerical Description of Effect Sizes\n\nCodesummary(mod_A_ols, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\nnum_gchild\n3\n8\n5\n0.004\n0.004\n-0.003\n0.010\n1\n\n\nnum_child\n1\n3\n2\n0.007\n0.004\n0.000\n0.015\n1\n\n\nnew_gchild - Yes:No\n2\n1\nNA\n-0.023\n0.009\n-0.038\n-0.009\n1\n\n\nmarital - Seperated:Married\n1\n2\nNA\n-0.002\n0.009\n-0.017\n0.013\n1\n\n\nmarital - Widowed:Married\n1\n3\nNA\n0.012\n0.009\n-0.002\n0.026\n1\n\n\nmarital - Never Married:Married\n1\n4\nNA\n0.006\n0.015\n-0.018\n0.031\n1\n\n\n\n\n\nNew grandchildren description: If we have two subjects of the same marital status, number of grandchildren and number of children, then if subject 1 does not have a new grandchildren and subject 2 does have a newgrand children, then our model estimates that subject 1 will have a log(sbp) that is 0.023 higher than subject 2 on average. The 90% confidence interval around that estimated effect on log(sbp) ranges from (0.0091, 0.038).\n\n8.8.4 Nomogram of Winning Model\n\nCodeggplot(Predict(mod_A_ols))\n\n\n\n\n\n\nCodeplot(nomogram(mod_A_ols, fun = exp, funlabel = \"SBP\"))\n\n\n\n\n\n\n\n\n8.8.5 Prediction for a New Subject\nHere, I’ll actually run two predictions, one for a subject with (1) and without (2) new_gchild with the same values of num_child (2), num_gchild (5) and marital status (widowed).\n\nCodenew_subjects &lt;- data.frame(num_child = c(2, 2), num_gchild = c(5, 5),\n             marital = c(\"Widowed\", \"Widowed\"), new_gchild = c(\"Yes\", \"No\"))\n\npreds1 &lt;- predict.lm(mod_A, newdata = new_subjects, \n                     interval = \"prediction\", level = 0.90)\n\nexp(preds1)\n\n       fit       lwr      upr\n1 127.6623  98.79885 164.9580\n2 130.6952 101.17939 168.8213"
  },
  {
    "objectID": "my first analysis/index.html#missingness-1",
    "href": "my first analysis/index.html#missingness-1",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n9.1 Missingness",
    "text": "9.1 Missingness\nAs explained in the data cleaning and 8.1.1 sections, we will work with complete cases and no missing data.\n\nWe have complete data for all of the logistic model predictors in 2000 (100%) of the 2000 rows in my data.\n\nAgain, we’ll assume missing values are MAR, and use the single imputation approach developed previously in Section 8.1.1. (optional) - We are missing XX values (XX%) for insert (variable name here). (optional with imputation)"
  },
  {
    "objectID": "my first analysis/index.html#model-y",
    "href": "my first analysis/index.html#model-y",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n9.2 Model Y",
    "text": "9.2 Model Y\nWe’ll predict Pr(diabetes = 1), the probability of having a low enough risk of diabetes to put the subject at risk, as a function of num_gchild, new_gchild, marital, and num_child.\n\n9.2.1 Fitting Model Y\n\nCodemod_Y &lt;- glm(diabetes ~ num_gchild + new_gchild + marital + num_child,\n            data = PhysTibble, family = binomial())\n\nddd &lt;- datadist(PhysTibble)\noptions(datadist = \"ddd\")\n\nmod_Y_lrm &lt;- lrm(diabetes == \"Yes\" ~ num_gchild + new_gchild + marital + num_child,\n                data = PhysTibble, x = TRUE, y = TRUE)\n\n\n\n9.2.2 Tidied Odds Ratio Estimates (Model Y)\n\nCodetidy(mod_Y, exponentiate = TRUE, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, \n         low90 = conf.low, high90 = conf.high, p = p.value) |&gt;\n  kable(digits = 3)\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n(Intercept)\n2.672\n0.138\n2.133\n3.358\n0.000\n\n\nnum_gchild\n0.965\n0.010\n0.949\n0.982\n0.001\n\n\nnew_gchildNo\n1.043\n0.120\n0.855\n1.270\n0.723\n\n\nmaritalSeperated\n0.862\n0.122\n0.705\n1.054\n0.222\n\n\nmaritalWidowed\n1.052\n0.118\n0.867\n1.279\n0.665\n\n\nmaritalNever Married\n1.191\n0.211\n0.847\n1.696\n0.407\n\n\nnum_child\n0.965\n0.030\n0.918\n1.014\n0.241\n\n\n\n\n\n\n9.2.3 Effects Plot (Model Y)\n\nCodeplot(summary(mod_Y_lrm, conf.int = 0.90))\n\n\n\n\n\n\n\n\n9.2.4 Summarizing Fit (Model Y)\n\nCodesummary(mod_Y_lrm, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\nnum_gchild\n3\n8\n5\n0.177\n0.051\n0.093\n0.261\n1\n\n\nOdds Ratio\n3\n8\n5\n1.194\nNA\n1.098\n1.299\n2\n\n\nnum_child\n1\n3\n2\n0.071\n0.060\n-0.029\n0.170\n1\n\n\nOdds Ratio\n1\n3\n2\n1.073\nNA\n0.972\n1.186\n2\n\n\nnew_gchild - Yes:No\n2\n1\nNA\n0.043\n0.120\n-0.155\n0.240\n1\n\n\nOdds Ratio\n2\n1\nNA\n1.043\nNA\n0.856\n1.271\n2\n\n\nmarital - Seperated:Married\n1\n2\nNA\n0.149\n0.122\n-0.052\n0.350\n1\n\n\nOdds Ratio\n1\n2\nNA\n1.161\nNA\n0.949\n1.419\n2\n\n\nmarital - Widowed:Married\n1\n3\nNA\n-0.051\n0.118\n-0.245\n0.143\n1\n\n\nOdds Ratio\n1\n3\nNA\n0.950\nNA\n0.782\n1.154\n2\n\n\nmarital - Never Married:Married\n1\n4\nNA\n-0.175\n0.211\n-0.521\n0.172\n1\n\n\nOdds Ratio\n1\n4\nNA\n0.840\nNA\n0.594\n1.188\n2\n\n\n\n\nCodemod_Y_lrm\n\nLogistic Regression Model\n\nlrm(formula = diabetes == \"Yes\" ~ num_gchild + new_gchild + marital + \n    num_child, data = PhysTibble, x = TRUE, y = TRUE)\n\n                      Model Likelihood       Discrimination    Rank Discrim.    \n                            Ratio Test              Indexes          Indexes    \nObs          2000    LR chi2     22.45       R2       0.016    C       0.571    \n FALSE       1339    d.f.            6      R2(6,2000)0.008    Dxy     0.143    \n TRUE         661    Pr(&gt; chi2) 0.0010    R2(6,1327.6)0.012    gamma   0.144    \nmax |deriv| 6e-08                            Brier    0.219    tau-a   0.063    \n\n                      Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept             -0.9828 0.1379 -7.13  &lt;0.0001 \nnum_gchild             0.0355 0.0102  3.47  0.0005  \nnew_gchild=No         -0.0425 0.1202 -0.35  0.7233  \nmarital=Seperated      0.1491 0.1221  1.22  0.2223  \nmarital=Widowed       -0.0511 0.1181 -0.43  0.6655  \nmarital=Never Married -0.1746 0.2107 -0.83  0.4072  \nnum_child              0.0354 0.0302  1.17  0.2415  \n\nCodeglance(mod_Y) |&gt;\n  mutate(df = nobs - df.residual - 1) |&gt;\n  select(AIC, BIC, df, df.residual, nobs) |&gt;\n  kable(digits = 1)\n\n\n\nAIC\nBIC\ndf\ndf.residual\nnobs\n\n\n2529.7\n2568.9\n6\n1993\n2000\n\n\n\n\nOur Nagelkerke R2 estimate for Model Y is 0.016, and our C statistic is estimated to be 0.571\n\n9.2.5 Confusion Matrix (Model Y)\nMy prediction rule for this confusion matrix is that the fitted value of Pr(diabetes = 1) needs to be greater than or equal to 0.65 for me to predict diabetes is 1, and otherwise I predict 0.\n\nCoderesY_aug &lt;- augment(mod_Y, type.predict = \"response\")\n\ncp1 &lt;- cutpointr(data = resY_aug, .fitted, diabetes,  \n                 pos_class = \"Yes\", neg_class = \"No\", \n                 method = maximize_metric, metric = sum_sens_spec) \n\nAssuming the positive class has lower x values\n\nCodecp1 |&gt; select(direction, optimal_cutpoint, method, sum_sens_spec) |&gt;  \n  gt() |&gt; tab_options(table.font.size = 24) |&gt;  \n  opt_stylize(style = 2, color = \"pink\") \n\n\n\n\n\ndirection\noptimal_cutpoint\nmethod\nsum_sens_spec\n\n\n&lt;=\n0.6491681\nmaximize_metric\n1.122952\n\n\n\n\nCodecm_Y &lt;- caret::confusionMatrix(\n  data = factor(resY_aug$.fitted &gt;= cp1$optimal_cutpoint),\n  reference = factor(resY_aug$diabetes == \"Yes\"),\n  positive = \"TRUE\")\n\ncm_Y\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE   278  218\n     TRUE   1061  443\n                                         \n               Accuracy : 0.3605         \n                 95% CI : (0.3394, 0.382)\n    No Information Rate : 0.6695         \n    P-Value [Acc &gt; NIR] : 1              \n                                         \n                  Kappa : -0.0924        \n                                         \n Mcnemar's Test P-Value : &lt;2e-16         \n                                         \n            Sensitivity : 0.6702         \n            Specificity : 0.2076         \n         Pos Pred Value : 0.2945         \n         Neg Pred Value : 0.5605         \n             Prevalence : 0.3305         \n         Detection Rate : 0.2215         \n   Detection Prevalence : 0.7520         \n      Balanced Accuracy : 0.4389         \n                                         \n       'Positive' Class : TRUE           \n                                         \n\n\nMy prediction rule is …\nHere are our results, tabulated nicely.\nModel Classification Rule Sensitivity Specificity Pos. Pred. Value Y Predicted Pr(diabetes = 1) &gt;= 0.65 0.670 0.208 0.294"
  },
  {
    "objectID": "my first analysis/index.html#non-linearity-1",
    "href": "my first analysis/index.html#non-linearity-1",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n9.3 Non-Linearity",
    "text": "9.3 Non-Linearity\n\nCodeplot(spearman2(diabetes == \"Yes\" ~ num_gchild + new_gchild + marital + num_child,\n            data = PhysTibble))\n\n\n\n\n\n\n\nOur Spearman p2 plot suggests the use of a non-linear term in num_gchild, so we’ll add a restricted cubic spline in num_gchild using 4 knots, which should add 2 degrees of freedom to our initial model.\nNext, the num_child variable seems to be a good choice, so we’ll add an interaction between num_child and the main effect of num_gchild, which will add one more degrees of freedom to our model Y."
  },
  {
    "objectID": "my first analysis/index.html#model-z",
    "href": "my first analysis/index.html#model-z",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n9.4 Model Z",
    "text": "9.4 Model Z\nAs mentioned, our model Z will add 3 degrees of freedom through two non-linear terms, to model Y.\n\n9.4.1 Fitting Model Z\n\nCodemod_Z &lt;- glm(diabetes ~ num_child + rcs(num_gchild, 4) + new_gchild + \n                num_gchild %ia% num_child + marital,\n            data = PhysTibble, family = binomial())\n\nddd &lt;- datadist(PhysTibble)\noptions(datadist = \"ddd\")\n\nmod_Z_lrm &lt;- lrm(diabetes == \"Yes\" ~ num_child + rcs(num_gchild, 4) + new_gchild + \n                num_gchild %ia% num_child + marital,\n                 data = PhysTibble, x = TRUE, y = TRUE)\n\n\n\n9.4.2 Tidied Odds Ratio Estimates (Model Z)\n\nCodetidy(mod_Z, exponentiate = TRUE, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, \n         low90 = conf.low, high90 = conf.high, p = p.value) |&gt;\n  kable(digits = 3)\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n(Intercept)\n2.457\n0.231\n1.685\n3.608\n0.000\n\n\nnum_child\n0.885\n0.056\n0.807\n0.970\n0.030\n\n\nrcs(num_gchild, 4)num_gchild\n1.053\n0.074\n0.932\n1.189\n0.482\n\n\nrcs(num_gchild, 4)num_gchild’\n0.475\n0.464\n0.222\n1.020\n0.109\n\n\nrcs(num_gchild, 4)num_gchild’’\n5.305\n1.029\n0.975\n28.800\n0.105\n\n\nnew_gchildNo\n1.045\n0.120\n0.856\n1.272\n0.714\n\n\nnum_gchild %ia% num_child\n1.009\n0.005\n1.001\n1.018\n0.070\n\n\nmaritalSeperated\n0.855\n0.123\n0.699\n1.047\n0.203\n\n\nmaritalWidowed\n1.050\n0.118\n0.864\n1.276\n0.683\n\n\nmaritalNever Married\n1.220\n0.211\n0.867\n1.739\n0.346\n\n\n\n\n\n\n9.4.3 Effects Plot (Model Z)\n\nCodeplot(summary(mod_Z_lrm, conf.int = 0.90))\n\n\n\n\n\n\n\n\n9.4.4 Summarizing Fit (Model Z)\n\nCodesummary(mod_Z_lrm, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\nnum_child\n1\n3\n2\n0.152\n0.075\n0.028\n0.276\n1\n\n\nOdds Ratio\n1\n3\n2\n1.164\nNA\n1.028\n1.318\n2\n\n\nnum_gchild\n3\n8\n5\n0.285\n0.108\n0.108\n0.463\n1\n\n\nOdds Ratio\n3\n8\n5\n1.330\nNA\n1.114\n1.589\n2\n\n\nnew_gchild - Yes:No\n2\n1\nNA\n0.044\n0.120\n-0.154\n0.242\n1\n\n\nOdds Ratio\n2\n1\nNA\n1.045\nNA\n0.857\n1.274\n2\n\n\nmarital - Seperated:Married\n1\n2\nNA\n0.156\n0.123\n-0.046\n0.358\n1\n\n\nOdds Ratio\n1\n2\nNA\n1.169\nNA\n0.955\n1.431\n2\n\n\nmarital - Widowed:Married\n1\n3\nNA\n-0.048\n0.118\n-0.243\n0.146\n1\n\n\nOdds Ratio\n1\n3\nNA\n0.953\nNA\n0.784\n1.158\n2\n\n\nmarital - Never Married:Married\n1\n4\nNA\n-0.199\n0.211\n-0.547\n0.149\n1\n\n\nOdds Ratio\n1\n4\nNA\n0.820\nNA\n0.579\n1.160\n2\n\n\n\n\nCodemod_Z_lrm\n\nLogistic Regression Model\n\nlrm(formula = diabetes == \"Yes\" ~ num_child + rcs(num_gchild, \n    4) + new_gchild + num_gchild %ia% num_child + marital, data = PhysTibble, \n    x = TRUE, y = TRUE)\n\n                       Model Likelihood       Discrimination    Rank Discrim.    \n                             Ratio Test              Indexes          Indexes    \nObs           2000    LR chi2     28.70       R2       0.020    C       0.571    \n FALSE        1339    d.f.            9      R2(9,2000)0.010    Dxy     0.143    \n TRUE          661    Pr(&gt; chi2) 0.0007    R2(9,1327.6)0.015    gamma   0.144    \nmax |deriv| 0.0002                            Brier    0.218    tau-a   0.063    \n\n                       Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept              -0.8991 0.2313 -3.89  0.0001  \nnum_child               0.1217 0.0560  2.17  0.0297  \nnum_gchild             -0.0520 0.0739 -0.70  0.4820  \nnum_gchild'             0.7434 0.4637  1.60  0.1089  \nnum_gchild''           -1.6686 1.0288 -1.62  0.1048  \nnew_gchild=No          -0.0441 0.1203 -0.37  0.7142  \nnum_gchild * num_child -0.0091 0.0050 -1.81  0.0701  \nmarital=Seperated       0.1562 0.1227  1.27  0.2031  \nmarital=Widowed        -0.0484 0.1184 -0.41  0.6825  \nmarital=Never Married  -0.1990 0.2114 -0.94  0.3465  \n\nCodeglance(mod_Z) |&gt;\n  mutate(df = nobs - df.residual - 1) |&gt;\n  select(AIC, BIC, df, df.residual, nobs) |&gt;\n  kable(digits = 1)\n\n\n\nAIC\nBIC\ndf\ndf.residual\nnobs\n\n\n2529.4\n2585.4\n9\n1990\n2000\n\n\n\n\n\n9.4.5 Confusion Matrix (Model Z)\nAs in Model Y, my prediction rule for this confusion matrix is that the fitted value of Pr(diabetes = 1) needs to be greater than or equal to 0.65 for me to predict diabetes is 1, and otherwise I predict 0.\nAgain, we augment our PhyTibble data to include predicted probabilities of (diabetes = 1) from Model Z.\n\nCoderesZ_aug &lt;- augment(mod_Z, type.predict = \"response\")\n\ncm_Z &lt;- caret::confusionMatrix(\n  data = factor(resZ_aug$.fitted &gt;= 0.65),\n  reference = factor(resZ_aug$diabetes == \"Yes\"),\n  positive = \"TRUE\")\n\ncm_Z\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE   373  257\n     TRUE    966  404\n                                          \n               Accuracy : 0.3885          \n                 95% CI : (0.3671, 0.4103)\n    No Information Rate : 0.6695          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : -0.0867         \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.6112          \n            Specificity : 0.2786          \n         Pos Pred Value : 0.2949          \n         Neg Pred Value : 0.5921          \n             Prevalence : 0.3305          \n         Detection Rate : 0.2020          \n   Detection Prevalence : 0.6850          \n      Balanced Accuracy : 0.4449          \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\nHere are our results comparing classification performance by models Y and Z.\nModel Classification Rule Sensitivity Specificity Pos. Pred. Value Y Predicted Pr(HDL_RISK = 1) &gt;= 0.65 0.670 0.208 0.294 Z Predicted Pr(HDL_RISK = 1) &gt;= 0.65 0.611 0.279 0.295"
  },
  {
    "objectID": "my first analysis/index.html#validating-models-y-and-z",
    "href": "my first analysis/index.html#validating-models-y-and-z",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n9.5 Validating Models Y and Z",
    "text": "9.5 Validating Models Y and Z\nWe will use the validate() function from the rms package to validate our lrm fits.\n\nCodeset.seed(4323); (valY &lt;- validate(mod_Y_lrm))\n\n          index.orig training    test optimism index.corrected  n\nDxy           0.1430   0.1515  0.1228   0.0286          0.1144 40\nR2            0.0155   0.0195  0.0124   0.0070          0.0085 40\nIntercept     0.0000   0.0000 -0.1359   0.1359         -0.1359 40\nSlope         1.0000   1.0000  0.8194   0.1806          0.8194 40\nEmax          0.0000   0.0000  0.0690   0.0690          0.0690 40\nD             0.0107   0.0136  0.0085   0.0051          0.0056 40\nU            -0.0010  -0.0010  0.0003  -0.0013          0.0003 40\nQ             0.0117   0.0146  0.0082   0.0064          0.0053 40\nB             0.2188   0.2189  0.2196  -0.0007          0.2195 40\ng             0.2333   0.2636  0.2118   0.0518          0.1815 40\ngp            0.0522   0.0588  0.0472   0.0116          0.0406 40\n\nCodeset.seed(4324); (valZ &lt;- validate(mod_Z_lrm))\n\n          index.orig training    test optimism index.corrected  n\nDxy           0.1429   0.1664  0.1278   0.0386          0.1043 40\nR2            0.0198   0.0277  0.0153   0.0124          0.0074 40\nIntercept     0.0000   0.0000 -0.1688   0.1688         -0.1688 40\nSlope         1.0000   1.0000  0.7506   0.2494          0.7506 40\nEmax          0.0000   0.0000  0.0951   0.0951          0.0951 40\nD             0.0138   0.0196  0.0106   0.0091          0.0048 40\nU            -0.0010  -0.0010  0.0011  -0.0021          0.0011 40\nQ             0.0148   0.0206  0.0094   0.0112          0.0036 40\nB             0.2180   0.2162  0.2192  -0.0031          0.2211 40\ng             0.2842   0.3286  0.2440   0.0846          0.1996 40\ngp            0.0629   0.0722  0.0540   0.0182          0.0447 40\n\n\n\n9.5.1 Validated Nagelkerke \\(R^2\\) and \\(C\\) statistics for each model\nC = 0.5 + Dxy/2\nModel Validated R2 Validated C AIC BIC df Y 0.0085 0.5572 2529.7 2568.9 6\nZ 0.0074 0.5521 2529.4 2585.4 9"
  },
  {
    "objectID": "my first analysis/index.html#final-logistic-regression-model",
    "href": "my first analysis/index.html#final-logistic-regression-model",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n9.6 Final Logistic Regression Model",
    "text": "9.6 Final Logistic Regression Model\nI prefer Model Y, because of its slightly better validated R2, and validated C statistic, despite the fact that Model Z has a slightly lower AIC and BIC and that Model Z also has a slightly higher positive predictive value. It’s pretty close, though.\n\n9.6.1 Winning Model’s Parameter Estimates\n\nCodemod_Y_lrm\n\nLogistic Regression Model\n\nlrm(formula = diabetes == \"Yes\" ~ num_gchild + new_gchild + marital + \n    num_child, data = PhysTibble, x = TRUE, y = TRUE)\n\n                      Model Likelihood       Discrimination    Rank Discrim.    \n                            Ratio Test              Indexes          Indexes    \nObs          2000    LR chi2     22.45       R2       0.016    C       0.571    \n FALSE       1339    d.f.            6      R2(6,2000)0.008    Dxy     0.143    \n TRUE         661    Pr(&gt; chi2) 0.0010    R2(6,1327.6)0.012    gamma   0.144    \nmax |deriv| 6e-08                            Brier    0.219    tau-a   0.063    \n\n                      Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept             -0.9828 0.1379 -7.13  &lt;0.0001 \nnum_gchild             0.0355 0.0102  3.47  0.0005  \nnew_gchild=No         -0.0425 0.1202 -0.35  0.7233  \nmarital=Seperated      0.1491 0.1221  1.22  0.2223  \nmarital=Widowed       -0.0511 0.1181 -0.43  0.6655  \nmarital=Never Married -0.1746 0.2107 -0.83  0.4072  \nnum_child              0.0354 0.0302  1.17  0.2415  \n\n\n\n9.6.2 Plot of Effect Sizes for Winning Model\n\nCodeplot(summary(mod_Y_lrm, conf.int = 0.90))\n\n\n\n\n\n\n\n\n9.6.3 Numerical Description of Effect Sizes\n\nCodesummary(mod_Y_lrm, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\nnum_gchild\n3\n8\n5\n0.177\n0.051\n0.093\n0.261\n1\n\n\nOdds Ratio\n3\n8\n5\n1.194\nNA\n1.098\n1.299\n2\n\n\nnum_child\n1\n3\n2\n0.071\n0.060\n-0.029\n0.170\n1\n\n\nOdds Ratio\n1\n3\n2\n1.073\nNA\n0.972\n1.186\n2\n\n\nnew_gchild - Yes:No\n2\n1\nNA\n0.043\n0.120\n-0.155\n0.240\n1\n\n\nOdds Ratio\n2\n1\nNA\n1.043\nNA\n0.856\n1.271\n2\n\n\nmarital - Seperated:Married\n1\n2\nNA\n0.149\n0.122\n-0.052\n0.350\n1\n\n\nOdds Ratio\n1\n2\nNA\n1.161\nNA\n0.949\n1.419\n2\n\n\nmarital - Widowed:Married\n1\n3\nNA\n-0.051\n0.118\n-0.245\n0.143\n1\n\n\nOdds Ratio\n1\n3\nNA\n0.950\nNA\n0.782\n1.154\n2\n\n\nmarital - Never Married:Married\n1\n4\nNA\n-0.175\n0.211\n-0.521\n0.172\n1\n\n\nOdds Ratio\n1\n4\nNA\n0.840\nNA\n0.594\n1.188\n2\n\n\n\n\n\nNumber of grandchildren description: If we have two subjects of the same marital status, number of children and new grandchildren status, but a subject 1 has a number of grandchildren of 3 and subject 2 number of grandchildren of 8, then our model estimates that subject 2 will have 1.194 times the odds (90% CI: 1.098, 1.299 ) that subject 1 has of being at risk of diabetes.\n\n9.6.4 Plot of ROC Curve for Winning Model\nperformance_roc(mod_Y)\nplot(performance_roc(mod_Y)) + labs(title = glue(“Model mod_Y: C statistic =”, round_half_up(as.numeric(performance_roc(mod_Y)),3)))\n\n9.6.5 Validated \\(R^2\\) and \\(C\\) statistic for Winning Model\nAs we saw in Section 9.5.1,\nthe validated R2 statistic for Model Y is 0.0085, and\nthe validated C statistic for Model Y is 0.5572.\n\n9.6.6 Nomogram of Winning Model\n\nCodeplot(nomogram(mod_Y_lrm, fun = plogis, \n              funlabel = \"Pr(diabetes = 1)\"))\n\n\n\n\n\n\n\n\n9.6.7 Predictions for Two New Subjects\nI will create a predicted Pr(diabetes = 1) for two subjects with the same values of num_child (2), new_gchild (Yes) and marital status (Widowed). The first will have a num_gchild of 3, and the second will have a num_gchild of 8.\n\nCodenew_subjects &lt;- data.frame(num_child = c(3, 8), num_gchild = c(5, 5),\n             marital = c(\"Widowed\", \"Widowed\"), new_gchild = c(\"Yes\", \"Yes\"))\n\npreds2 &lt;- predict.lm(mod_Y, newdata = new_subjects, \n                     interval = \"prediction\", level = 0.90)\n\nWarning in predict.lm(mod_Y, newdata = new_subjects, interval = \"prediction\", : Assuming constant prediction variance even though model fit is weighted\n\nCodeexp(preds2)\n\n       fit       lwr       upr\n1 2.117765 0.4004501 11.199717\n2 1.774273 0.3266542  9.637243"
  },
  {
    "objectID": "my first analysis/index.html#answering-my-research-questions",
    "href": "my first analysis/index.html#answering-my-research-questions",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n10.1 Answering My Research Questions",
    "text": "10.1 Answering My Research Questions\n\n10.1.1 Question 1 (with Answer)\nDoes an increased presence of children and grandchildren directly affect a person’s Systolic Blood Pressure while accounting for marital status?\nAccording to model A, having a new grandchildren increases the log(sbp) by 0.023 on average when accounting for marital status, number of grandchildren and number of children with a 90% confidence interval around that estimated effect on log(sbp) ranging from (0.0091, 0.038).\n\n10.1.2 Question 2 (with Answer)\nIs there an increased risk for diabetes in those with either less children or grandchildren and how does this change when factoring in a person’s current marital status?\nAccording to model Y, the estimated odds ratio of having diabetes is 0.97 for every increased number of grandchildren when adjusting for marital status, number of children and new grandchildren status with a 90% CI 0.95, 0.98."
  },
  {
    "objectID": "my first analysis/index.html#thoughts-on-project-a",
    "href": "my first analysis/index.html#thoughts-on-project-a",
    "title": "Predicting Health Conditions in Adults over 50 with Family Structure",
    "section": "\n10.2 Thoughts on Project A",
    "text": "10.2 Thoughts on Project A\n\n10.2.1 Question 2\nWhat do you wish you’d known at the start of this process that you know now, and why?\nWe wish we knew that we would encounter lots of challenging during the review process of the portfolio because we would have designated more time in improving the details of the analysis.\n\n10.2.2 Question 4\nWhat was the most useful thing you learned while doing the project, and why?\nThe most useful thing for us was to being able to use a database not seen during class because it allowed us to have a sense of what it is actually like to approach a public data set for a research question analysis without previous experience."
  }
]